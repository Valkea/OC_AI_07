{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "075244a7-1459-4c89-8d3a-ef6b76941c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valkea/Dev/OpenClassrooms/Projets_AI/P7/venvP7/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-08-10 13:46:44.473866: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:46:44.473908: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pathlib \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f485fde-99b5-4060-a080-b690d4b65c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" # 5 labels\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\" # 3 labels mais fait pour\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\" # 2 labels\n",
    "# finiteautomata/bertweet-base-sentiment-analysis ??\n",
    "# \"distilbert-base-uncased\" --> fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89194639-29e0-4e24-a1c3-f1521e1f4a9d",
   "metadata": {},
   "source": [
    "Un **transformer** est un modèle qui utilise l'attention pour augmenter la vitesse à laquelle il peut être entrainé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab3d3e0-3a6a-40ba-9386-c469e935a6a7",
   "metadata": {},
   "source": [
    "# 1 - pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa086465-b1a8-496b-8f41-9fb79110b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 13:46:48.532106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-08-10 13:46:48.532136: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-08-10 13:46:48.532157: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (valkea-XPS): /proc/driver/nvidia/version does not exist\n",
      "2022-08-10 13:46:48.532671: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", top_k=10, model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e2c616-b870-41bd-b31f-d41732e16254",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"I really love it\", \"I really hate it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7939c0-78e0-4375-9064-fec394edafeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'POSITIVE', 'score': 0.9998788833618164},\n",
       "  {'label': 'NEGATIVE', 'score': 0.00012104477355023846}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.9996116757392883},\n",
       "  {'label': 'POSITIVE', 'score': 0.0003883188182953745}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f516005-eb3a-4d84-8b22-2b92c8dd387a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe02358f-4686-461e-bba1-a0520962dd6c",
   "metadata": {},
   "source": [
    "# 2 - AutoModel & AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a129da-bc64-4c88-b4c3-e1e99edd2ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a37e38-bf56-4a76-82d2-58a0ca33b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer([\"I really hate it\", \"I really love it\"], \n",
    "          truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497a017a-1e5c-4675-a599-4cc6e3f48221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2428, 5223, 2009,  102],\n",
       "       [ 101, 1045, 2428, 2293, 2009,  102]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 6), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ebaa8b-e8fe-4f3d-9d03-9c03d1c3128c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 4.3365107, -3.516785 ],\n",
       "       [-4.327714 ,  4.691517 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs = tf_model(data)\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4199af95-c8ec-4062-9c64-dc86d5cbdf3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[9.9961168e-01, 3.8831888e-04],\n",
       "       [1.2104454e-04, 9.9987888e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "tf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655871e-3107-405e-b2b6-cc4d3349713a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ce31420-d3cc-4c48-a661-9919e160035d",
   "metadata": {},
   "source": [
    "# 3 - Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd26e73-8916-42a9-9e09-8ff7396aa6cc",
   "metadata": {},
   "source": [
    "### From CSV 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2052c218-598e-41f1-9bc2-905c83354251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration data-3d6d34e60550ee5b\n",
      "Reusing dataset csv (/home/valkea/.cache/huggingface/datasets/csv/data-3d6d34e60550ee5b/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = load_dataset(\n",
    "    'data',\n",
    "    'csv',\n",
    "    split='train',\n",
    "    data_files={'data_nlp_1563108.csv'},\n",
    "    column_names=['text', 'target'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa412718-7e40-471f-aa36-07dbb2f3d2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'target', '__index_level_0__', '__index_level_1__', '__index_level_2__', '__index_level_3__', '__index_level_4__', '__index_level_5__', '__index_level_6__'],\n",
       "    num_rows: 1452792\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03f1df94-eb17-446d-ac82-67be3046a7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'target', '__index_level_0__', '__index_level_1__', '__index_level_2__', '__index_level_3__', '__index_level_4__', '__index_level_5__', '__index_level_6__'],\n",
       "        num_rows: 1307512\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'target', '__index_level_0__', '__index_level_1__', '__index_level_2__', '__index_level_3__', '__index_level_4__', '__index_level_5__', '__index_level_6__'],\n",
       "        num_rows: 145280\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit1 = dataset1.train_test_split(test_size=0.1)\n",
    "dsplit1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5507197f-c130-456f-86d8-7e5b9e43048a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'bummer shoulda get day d',\n",
       " 'target': '$ url$ - awww , that be a bummer . you shoulda get david carr of third day to do it . ; d',\n",
       " '__index_level_0__': '0',\n",
       " '__index_level_1__': \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\",\n",
       " '__index_level_2__': \"$URL$ - Awww, that's a bummer. You shoulda got David Carr of Third Day to do it. ;D\",\n",
       " '__index_level_3__': \"$ url$ - awww , that 's a bummer . you shoulda got david carr of third day to do it . ; d\",\n",
       " '__index_level_4__': 'bummer shoulda got day d',\n",
       " '__index_level_5__': 'bummer shoulda got day d',\n",
       " '__index_level_6__': 'bummer shoulda get day d'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315a1b98-ee4e-434a-a951-055f968141be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'get promotion favoooor',\n",
       " 'target': 'just get a promotion ! favoooor !',\n",
       " '__index_level_0__': '1',\n",
       " '__index_level_1__': 'Just got a promotion!  favoooor!',\n",
       " '__index_level_2__': 'Just got a promotion! favoooor!',\n",
       " '__index_level_3__': 'just got a promotion ! favoooor !',\n",
       " '__index_level_4__': 'got promotion favoooor',\n",
       " '__index_level_5__': 'got promotion favoooor',\n",
       " '__index_level_6__': 'get promotion favoooor'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit1['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79dc7d-345d-4d4b-9b44-0f4ec675cd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88974e0c-69ba-41b2-b990-2ba5a45590ee",
   "metadata": {},
   "source": [
    "### From CSV 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d5230b1-70b9-4477-a1b1-fd3c1a2b4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c72cb7284115c351\n",
      "Reusing dataset csv (/home/valkea/.cache/huggingface/datasets/csv/default-c72cb7284115c351/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = Dataset.from_csv('data/data_nlp_1563108.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bfb9689-ce29-4ac0-88c1-250093715e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target', 'text', 'text_clean', 'tokens', 'tokens_filtered_advanced', 'tokens_filtered_simple', 'lemmas_filtered_advanced', 'lemmas_filtered_simple', 'lemmas_not_filtered'],\n",
       "    num_rows: 1452791\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa1f925-7525-41fd-abd0-95cf2a4679f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['target', 'text', 'text_clean', 'tokens', 'tokens_filtered_advanced', 'tokens_filtered_simple', 'lemmas_filtered_advanced', 'lemmas_filtered_simple', 'lemmas_not_filtered'],\n",
       "        num_rows: 1307511\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['target', 'text', 'text_clean', 'tokens', 'tokens_filtered_advanced', 'tokens_filtered_simple', 'lemmas_filtered_advanced', 'lemmas_filtered_simple', 'lemmas_not_filtered'],\n",
       "        num_rows: 145280\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit2 = dataset2.train_test_split(test_size=0.1)\n",
    "dsplit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4b93215-857c-4591-8f49-54044ce4c72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0,\n",
       " 'text': \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\",\n",
       " 'text_clean': \"is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!\",\n",
       " 'tokens': \"is upset that he ca n't update his facebook by texting it ... and might cry as a result school today also . blah !\",\n",
       " 'tokens_filtered_advanced': 'upset update facebook texting cry result school today',\n",
       " 'tokens_filtered_simple': 'upset update facebook texting cry result school today',\n",
       " 'lemmas_filtered_advanced': 'upset update facebook texte cry result school today',\n",
       " 'lemmas_filtered_simple': 'upset update facebook texte cry result school today',\n",
       " 'lemmas_not_filtered': 'be upset that he can not update his facebook by texte it ... and might cry as a result school today also . blah !'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96c3d7b6-ebd7-4650-abd8-a3da2b84cd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0,\n",
       " 'text': \"@theoopsgirl glad you had good time! Missed you as well, my silly manager going on holiday so I couldn't go! \",\n",
       " 'text_clean': \"glad you had good time! Missed you as well, my silly manager going on holiday so I couldn't go!\",\n",
       " 'tokens': \"glad you had good time ! missed you as well , my silly manager going on holiday so i could n't go !\",\n",
       " 'tokens_filtered_advanced': 'glad good time missed silly manager going holiday',\n",
       " 'tokens_filtered_simple': 'glad good time missed silly manager going holiday',\n",
       " 'lemmas_filtered_advanced': 'glad good time miss silly manager go holiday',\n",
       " 'lemmas_filtered_simple': 'glad good time miss silly manager go holiday',\n",
       " 'lemmas_not_filtered': 'glad you have good time ! miss you as well , my silly manager go on holiday so I could not go !'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit2['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59767493-25f1-4c1b-bd11-b513417fbd23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31a96e86-5554-4913-a2a2-0bf790297cbb",
   "metadata": {},
   "source": [
    "### From Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d61ffce1-e507-4c70-95d1-3063810ade98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 9106,
     "status": "ok",
     "timestamp": 1659976360486,
     "user": {
      "displayName": "Valkea",
      "userId": "01476199649418572392"
     },
     "user_tz": -120
    },
    "id": "ddcbcb93-38d4-4eb1-a0ad-58ae67f6a4c8",
    "outputId": "ca781965-b8f2-466e-b6e0-2bf55099127b",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1452791, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I dont have a background for my twitter stream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I did many things but missed tweeting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "label                                                   \n",
       "0      I dont have a background for my twitter stream...\n",
       "0                 I did many things but missed tweeting "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(100000, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_work = pd.read_csv(\n",
    "    pathlib.Path(pathlib.Path().absolute(), 'data', 'data_nlp_1563108.csv'), \n",
    "    usecols=['target', 'text'],\n",
    "    encoding='ISO-8859-1',\n",
    "    #nrows=100000,\n",
    ")\n",
    "data_work.rename(columns={'text':'text', 'target':'label'}, inplace=True)\n",
    "display(data_work.head(2), data_work.shape)\n",
    "\n",
    "# Select samples\n",
    "sample_size = 100000\n",
    "data_work = data_work.groupby('label', group_keys=False).apply(lambda x: x.sample(sample_size//2, random_state=random_seed))\n",
    "data_work.set_index('label', inplace=True, drop=True)\n",
    "display(data_work.head(2), data_work.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f4ec3-b3ad-4f67-95e0-76e773a07695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23c5a5d9-6c87-4dd8-9065-495e094391bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = Dataset.from_pandas(data_work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c90bf4db-b6ab-4dd7-92a7-a71d6d24c6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100000\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37a06b98-badd-47da-8abb-cf01809227cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 90000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit3 = dataset3.train_test_split(test_size=0.1)\n",
    "dsplit3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1566cdc1-5399-4c58-94e5-5b8d444f7037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I did many things but missed tweeting ', 'label': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96a82cb9-5237-4d76-a56c-08bdbbbcb3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Still awake, going to sleep soon I hope. Just getting some reading done  Not skyping though ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit3['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f4aba-2516-45b7-8460-e90d7c30aba3",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd007bbb-e42d-458b-81c2-181c30f57de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6894376-b956-4651-a5de-122ab37db869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d4fcc-0a37-4e37-8347-cb1931e3a2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432da3e-05a3-4922-b807-b817ceed864f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d54fab13-0243-4896-9bf9-f7003bd31904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I dont have a background for my twitter stream...does anyone ever check those things?? and if they do...what should mine be? help ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4c72e42-6159-4768-8ec4-50b6e74951a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Still awake, going to sleep soon I hope. Just getting some reading done  Not skyping though ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit3['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "055da62f-9186-4008-8196-cd2450b3115c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Still awake, going to sleep soon I hope. Just getting some reading done  Not skyping though '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplit3['test'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda21490-551c-44a3-b695-5399a4ae4aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "097e4dfe-b69f-453a-b8de-7ffbf06f60db",
   "metadata": {},
   "source": [
    "### Un test avec **quelques textes** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafc004-25a4-41b6-bf8a-800e2d71f00a",
   "metadata": {},
   "source": [
    "#### avec le pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d65283ac-f1de-4062-a993-a076b952b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = [\"I hate this stuff\", \"I really love it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "174695bd-fc8d-4d62-80dc-50f9fe6b8802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 0.999596893787384},\n",
       "  {'label': 'POSITIVE', 'score': 0.0004031736170873046}],\n",
       " [{'label': 'POSITIVE', 'score': 0.9998788833618164},\n",
       "  {'label': 'NEGATIVE', 'score': 0.00012104477355023846}]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_outputs = sentiment_pipeline(test_txt)\n",
    "pp_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "527935e8-cd7b-453c-93fd-4541318492da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0004031736170873046, 0.9998788833618164]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_proba = [[y['score'] for y in x if y['label'] == \"POSITIVE\"][0] for x in pp_outputs]\n",
    "y_preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f742925-b14d-400d-9a05-a2c802c569cd",
   "metadata": {},
   "source": [
    "#### avec les Auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f418d75-f8fe-441d-9bb2-708bb423b895",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = [\"I hate this stuff\", \"I really love it\"]\n",
    "test_tokens = tokenizer(test_txt, truncation=True, padding=\"max_length\", max_length=65, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "504c0548-c255-449c-88b8-4c08623a06b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 4.3156376, -3.5001032],\n",
       "       [-4.3277135,  4.691517 ]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_outputs = tf_model(test_tokens)\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7798d167-3224-4446-b158-50d11079e587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[9.9959689e-01, 4.0317344e-04],\n",
       "       [1.2104466e-04, 9.9987888e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "tf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfac425f-a3c4-44d9-beb4-5a63402a1d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00040317344, 0.9998789]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_proba = tf_predictions.numpy()\n",
    "y_preds_proba = [x[1] for x in y_preds_proba]\n",
    "y_preds_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc0e07-e9a3-4230-9901-9fedeedb2a96",
   "metadata": {},
   "source": [
    "### Un test avec **de nombreux textes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad89b1-392a-4eb9-a044-81d8334a444d",
   "metadata": {},
   "source": [
    "#### avec le pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07549537-36b2-40e9-a74f-84732db2bff8",
   "metadata": {},
   "source": [
    "test_txt = [\"I hate this stuff\", \"I really love it\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7fa7f3c-e6ec-4747-b9a9-74ba230d4b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I dont have a background for my twitter stream...does anyone ever check those things?? and if they do...what should mine be? help ',\n",
       " 'I did many things but missed tweeting ',\n",
       " '@JoAnneJoyM most probably - eww all that pasty white skin on show. ',\n",
       " 'Ohh, how bad I sleep ',\n",
       " '@LornA_AlicE hell yeah u were texting me i remember, i was at my nans n u told me tragic news! boo hoo! n u went to amsterdam without me! ']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_txt = data_work.text[:100].to_list()\n",
    "test_txt[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4038fd85-6f48-40ed-83f3-da6a963fdf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 0.9994959831237793},\n",
       "  {'label': 'POSITIVE', 'score': 0.0005040332325734198}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.9947373270988464},\n",
       "  {'label': 'POSITIVE', 'score': 0.005262717138975859}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.9963040351867676},\n",
       "  {'label': 'POSITIVE', 'score': 0.003695984371006489}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.9995403289794922},\n",
       "  {'label': 'POSITIVE', 'score': 0.00045966703328303993}],\n",
       " [{'label': 'NEGATIVE', 'score': 0.9946225881576538},\n",
       "  {'label': 'POSITIVE', 'score': 0.005377411376684904}]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_outputs = sentiment_pipeline(test_txt)\n",
    "pp_outputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "436051b5-7e44-4ede-b979-9a84f613d3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0005040332325734198,\n",
       " 0.005262717138975859,\n",
       " 0.003695984371006489,\n",
       " 0.00045966703328303993,\n",
       " 0.005377411376684904]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_proba = [[y['score'] for y in x if y['label'] == \"POSITIVE\"][0] for x in pp_outputs]\n",
    "y_preds_proba[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3906a8-82da-4426-b6a8-66b977361281",
   "metadata": {},
   "source": [
    "#### avec les Auto"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4e1c55d-502f-40fc-b8d0-2083180ea566",
   "metadata": {},
   "source": [
    "test_tokens = tokenizer(test_txt, truncation=True, padding=\"max_length\", return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5f06d83-7d62-4522-b723-1c9f0c674d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Encoding(num_tokens=65, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK\n",
    "test_txt = data_work.text[:100].to_list()\n",
    "test_tokens = tokenizer(test_txt, truncation=True, padding=\"max_length\", max_length=65, return_tensors=\"tf\")\n",
    "test_tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "318ed094-c1bf-49ae-ac0d-54bb7dd6d365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(100, 65), dtype=int32, numpy=\n",
       "array([[  101,  1045,  2123, ...,     0,     0,     0],\n",
       "       [  101,  1045,  2106, ...,     0,     0,     0],\n",
       "       [  101,  1030, 23459, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1045,  8415, ...,     0,     0,     0],\n",
       "       [  101,  6295,  2035, ...,     0,     0,     0],\n",
       "       [  101, 19752,  2664, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(100, 65), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5ee826e-e5e0-45b1-a992-6f0453f0dead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(100, 65), dtype=int32, numpy=\n",
       "array([[  101,  2145,  8300, ...,     0,     0,     0],\n",
       "       [  101,  1045,  2123, ...,     0,     0,     0],\n",
       "       [  101,  1030,  8224, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 18168,  2290, ...,     0,     0,     0],\n",
       "       [  101, 17006,  1010, ...,     0,     0,     0],\n",
       "       [  101,  1030, 23787, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(100, 65), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK\n",
    "def preprocess_function(x):\n",
    "    return tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length=65, return_tensors=\"tf\")\n",
    "test_tokens = preprocess_function(dsplit3['test'][:100])\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d099eb3c-af32-4be9-a10a-f976e948e860",
   "metadata": {},
   "source": [
    "# ERROR\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "pre_tokenizer_columns = set(dsplit3['train'].features)\n",
    "test_tokens = dsplit3.map(preprocess_function, batched=True)\n",
    "tokenizer_columns = list(set(test_tokens[\"train\"].features) - pre_tokenizer_columns)\n",
    "print(\"Columns added by tokenizer:\", tokenizer_columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0764c87b-2587-41b9-9ee5-415e863aa7c8",
   "metadata": {},
   "source": [
    "# ERROR\n",
    "def preprocess_function(x):\n",
    "    return tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length=65, return_tensors=\"tf\")\n",
    "test_tokens = dsplit3['test'].map(preprocess_function, batched=True)\n",
    "test_tokens[:1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cce14dad-5d54-4c4a-a128-926e374fab59",
   "metadata": {},
   "source": [
    "# ERROR\n",
    "def preprocess_function(x):\n",
    "    return {\"input_ids\": tokenizer.encode(x['text'], truncation=True, padding=\"max_length\", max_length=65, return_tensors=\"tf\")}\n",
    "test_tokens = dsplit3['test'].map(preprocess_function, batched=True)\n",
    "test_tokens[:1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "543fc81d-293b-43cb-9c45-ef4ae5f4a5f3",
   "metadata": {},
   "source": [
    "# ERROR\n",
    "def preprocess_function(x):\n",
    "    return tokenizer(x['text'], truncation=True, padding=\"max_length\", max_length=65, return_tensors=\"tf\")\n",
    "test_tokens = dsplit3['test'].map(preprocess_function)#, batched=True)\n",
    "test_tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6047f2cc-2134-4146-b674-8bb858b69a0f",
   "metadata": {},
   "source": [
    "test_tokens = dsplit3['test'].map(\n",
    "    lambda x: {\"input_ids\": tokenizer.encode(\n",
    "        x['text'], padding=\"max_length\", truncation=True, max_length=512\n",
    "    )},\n",
    "    batched=True\n",
    ")\n",
    "test_tokens[:1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3913eda4-c578-4687-8356-241f34a610d3",
   "metadata": {},
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: flaubert_tokenizer.encode(\n",
    "        x['verbatim'], padding=max_length, truncation=True, max_length=512\n",
    "    ), batched=True) # this line raise the error \n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: {\"input_ids\": flaubert_tokenizer.encode(\n",
    "        x['verbatim'], padding=max_length, truncation=True, max_length=512\n",
    "    )},\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c87eee7-fe07-4f9a-a79c-007be9bd71f8",
   "metadata": {},
   "source": [
    "# ERROR (+ lent que la version au dessus)\n",
    "def tokenize_function(x):\n",
    "    return tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=65, return_tensors=\"tf\")\n",
    "\n",
    "test_tokens = dsplit3['test'].map(tokenize_function)#, batched=True)\n",
    "test_tokens[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f811e8e-662d-4297-af02-181f9ca61e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000\n",
    "t0 = time.perf_counter()\n",
    "tf_outputs = tf_model(test_tokens)\n",
    "#tf_outputs\n",
    "print(f\"Inference time: {(time.perf_counter() - t0):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cdda308b-c535-49b7-b328-7059a375fb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 3.64s\n"
     ]
    }
   ],
   "source": [
    "# 100\n",
    "t0 = time.perf_counter()\n",
    "tf_outputs = tf_model(test_tokens)\n",
    "#tf_outputs\n",
    "print(f\"Inference time: {(time.perf_counter() - t0):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a7ecc6d6-a64c-4e4f-936e-9c5e2da9f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 45.24s\n"
     ]
    }
   ],
   "source": [
    "# 1000\n",
    "t0 = time.perf_counter()\n",
    "tf_outputs = tf_model(test_tokens)\n",
    "#tf_outputs\n",
    "print(f\"Inference time: {(time.perf_counter() - t0):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "38cdd09f-eb20-445c-8f22-ba52a4674a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 2), dtype=float32, numpy=\n",
       "array([[9.9949592e-01, 5.0403317e-04],\n",
       "       [9.9473739e-01, 5.2627195e-03],\n",
       "       [9.9630404e-01, 3.6959790e-03],\n",
       "       [9.9954027e-01, 4.5966770e-04],\n",
       "       [9.9462253e-01, 5.3774193e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "tf_predictions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4d1962b1-e0c4-40bf-978c-0d2a5cd50ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0005040332, 0.0052627195, 0.003695979, 0.0004596677, 0.0053774193]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_proba = tf_predictions.numpy()\n",
    "y_preds_proba = [x[1] for x in y_preds_proba]\n",
    "y_preds_proba[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06f1a4-fb4c-40de-8da3-620771ef5b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae41d97-40b5-4416-9bf7-670a68dc8e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92356d1c-bdcd-4945-b5fe-b8c59c9c40ba",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716481a-61e2-4fd8-8228-fbc5c6508f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38788b2a-9240-4364-988d-fd3be1a3ec75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4d4f2-cee0-4d65-9e9b-cf4cac33a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    " \n",
    "tokenized_test = dataset_df_split['train'].map(preprocess_function, batched=True)\n",
    "tokenized_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b0294c-4044-4fdf-8a43-93a51c839c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ea58c-f305-44d1-bf24-1b66abd1ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(tokenized_test)\n",
    "tf_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915e94c-2b07-495a-9a20-c4cc80928729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae50be-77bf-43bd-abd1-e67f6f4ec013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    #return tokenizer(str(x), truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "    return tokenizer(str(x), truncation=True, padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc3f1140-610c-4b2b-a9a6-4b44272b57a2",
   "metadata": {},
   "source": [
    "dataset_df_tokens_train = dataset_df_split['train'].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667e118-f6c8-4358-80f4-22593fc46581",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = dataset_df_split['test'].map(encode , batched=True)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514eab45-c8b2-420d-9770-0ed1776e7150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50d56e-7219-4586-942e-6bf9fc85aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new index\n",
    "train_idx = [i for i in range(len(X_train.index))]\n",
    "test_idx = [i for i in range(len(X_test.index))]\n",
    "val_idx = [i for i in range(len(X_valid.index))]\n",
    "\n",
    "# Convert to numpy\n",
    "x_train = X_train.values[train_idx]\n",
    "x_test = X_test.values[test_idx]\n",
    "x_val = X_valid.values[val_idx]\n",
    "\n",
    "#y_train = y_train.values[train_idx]\n",
    "#y_test = y_test.values[test_idx]\n",
    "#y_val = y_valid.values[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28dac8-bd9e-4888-8482-71a5dcb7ea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "tr_tok = tokenizer(list(x_train), return_tensors='tf', truncation=True, padding=True, max_length=128)\n",
    "val_tok = tokenizer(list(x_val), return_tensors='tf', truncation=True, padding=True, max_length=128)\n",
    "test_tok = tokenizer(list(x_test), return_tensors='tf', truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435be76e-23e0-438d-b931-2fba988e94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_raw = model(test_tok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvP7",
   "language": "python",
   "name": "venvp7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
